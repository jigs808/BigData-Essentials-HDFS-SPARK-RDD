{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hadoop Installation.ipynb'   LICENSE\r\n"
     ]
    }
   ],
   "source": [
    "# use '!' to access shell commands in the notebook\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with installing prerequisites:\n",
    "\n",
    "### JAVA-JDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for incursio: \n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install default-java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In jupyter notebook, ``sudo`` command will then require PASS_KEY but you won't be able to ENTER your password. Therefore, below two are methods to input password to run ``sudo`` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use echo to give your PASS_KEY as an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done \n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "default-jdk is already the newest version (2:1.10-63ubuntu1~02).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 6 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!echo 'XXXXXXXX' | sudo -S apt-get install default-jdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save you password in a file and use it to provide PASS_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done \n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "default-jdk is already the newest version (2:1.10-63ubuntu1~02).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 6 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!sudo -S apt-get install default-jdk < /path/to/file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a Dedicated hadoop user\n",
    "\n",
    "It'll create a new user to install/run hadoop keeping it separated from other user accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for incursio: Sorry, try again.\n",
      "[sudo] password for incursio: \n",
      "sudo: 1 incorrect password attempt\n",
      "[sudo] password for incursio: Sorry, try again.\n",
      "[sudo] password for incursio: \n",
      "sudo: 1 incorrect password attempt\n"
     ]
    }
   ],
   "source": [
    "#!sudo -S addgroup hadoop < /path/to/file\n",
    "#!sudo adduser --ingroup hadoop hduser < path/to/file\n",
    "\n",
    "PASS_KEY = 'XXXXXXX'\n",
    "!echo $PASS_KEY | sudo -S addgroup hadoop\n",
    "!echo $PASS_KEY | sudo -S adduser --ingroup hadoop hduser "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring SSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "su: must be run from a terminal\r\n"
     ]
    }
   ],
   "source": [
    "!echo PASS_KEY | su - hduser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to execute following commands you will have to open the terminal.\n",
    "\n",
    "**Copy the following cell(s)**\n",
    "\n",
    "The below command will transfer the terminal access to newly created ``hduser``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "su - hduser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop requires an SSH access to manage nodes present all over the cluster. This command will generate an SSH key with empty(string) password.\n",
    "In general, it's not recommended to use empty(string) password, but since we don't want to enter the passphrase each time Hadoop connects to its nodes therefore, **leave it empty**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh-keygen -t rsa -P \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command creates a new file and appends generated key to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat .ssh/id_rsa.pub >> .ssh/authorized_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll need root access through ``hduser``, thus we'll add ``hduser`` to the list of sudoers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open nano to edit /etc/sudoers.tmp\n",
    "sudo visudo\n",
    "\n",
    "#and append the following at the EOF\n",
    "hduser ALL=(ALL:ALL) ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to disable IPv6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open system call configuration file\n",
    "sudo gedit /etc/sysctl.conf\n",
    "\n",
    "#and append the following at the EOF\n",
    "#disable ipv6  \n",
    "net.ipv6.conf.all.disable_ipv6 = 1  \n",
    "net.ipv6.conf.default.disable_ipv6 = 1   \n",
    "net.ipv6.conf.lo.disable_ipv6 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, **reboot** system. On boot, check whether the ipv6 has been disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it should return 1\n",
    "cat /proc/sys/net/ipv6/conf/all/disable_ipv6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell(s) in the terminal with access to ``hduser``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change directory\n",
    "cd /usr/local\n",
    "\n",
    "#download hadoop 3.1 in this directory\n",
    "#to download other/newer version check the link http://www-eu.apache.org/dist/hadoop/core/\n",
    "wget http://www-eu.apache.org/dist/hadoop/core/hadoop-3.1.0/hadoop-3.1.0.tar.gz\n",
    "\n",
    "#extract the tar file\n",
    "sudo tar xzf hadoop-3.1.0.tar.gz\n",
    "\n",
    "#rename it to hadoop\n",
    "sudo mv hadoop-3.1.0 hadoop\n",
    "\n",
    "#change the owner of files to hduser\n",
    "sudo chown -R hduser:hadoop hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hadoop and java home environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the editor\n",
    "sudo gedit ~/.bashrc\n",
    "\n",
    "#and append the following lines at the EOF\n",
    "\n",
    "export HADOOP_HOME=/usr/local/hadoop\n",
    "export JAVA_HOME=/usr/lib/jvm/default-java\n",
    "\n",
    "# Some convenient aliases and functions for running Hadoop-related commands  \n",
    "unalias fs &> /dev/null   \n",
    "alias fs=\"hadoop fs\"    \n",
    "unalias hls &> /dev/null  \n",
    "alias hls=\"fs -ls\"  \n",
    "\n",
    "# Add Hadoop bin/ directory to PATH  \n",
    "export PATH=$PATH:$HADOOP_HOME/bin\n",
    "# Add Hadoop sbin/ directory to PATH  \n",
    "export PATH=$PATH:$HADOOP_HOME/sbin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now edit the ``hadoop-env.sh`` and update JAVA_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You know the drill\n",
    "sudo gedit $HADOOP_HOME/etc/hadoop/hadoop-env.sh\n",
    "\n",
    "#update JAVA_HOME (don't append, instead search for likewise line of code, it might be in the comments!)\n",
    "export JAVA_HOME=/usr/lib/jvm/default-java\n",
    "\n",
    "#you can also update HADOOP_HOME (not necessary)\n",
    "export HADOOP_HOME=/usr/local/hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Hadoop Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standalone Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this mode, hadoop will be set to run in a non-distributed mode, as a single java process. Using this mode we can check whether the installation is upto-mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a directory to store input files\n",
    "mkdir $HADOOP_HOME/input\n",
    "\n",
    "#now to verify no-errors in the installation, we will run a sample using example jar file\n",
    "#copy all xml files to the newly created directory\n",
    "cp $HADOOP_HOME/etc/hadoop/*.xml $HADOOP_HOME/input\n",
    "\n",
    "#1st argument is the /path/to/hadoop command (required to run MapReduce)\n",
    "#2nd argument is jar, specifying MapReduce is in JAVA archive\n",
    "#3rd argument is come along MapReduce example, it name of jar can be version specific (check your file/version)\n",
    "#4th argument is grep, to execute regular expression example\n",
    "#5th argument is input directory, containing all the .xml files\n",
    "#6th argument is output directory, which will be created and will contain output files\n",
    "#7th argument is 'dfs[a-z.]+', basically the string to be searched\n",
    "$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.0.jar grep input output 'dfs[a-z.]+'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo Distributed Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this mode, hadoop runs on a single node in a pseudo distributed mode where each hadoop daemon run as separate java process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuring site xml(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the ``tmp`` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo mkdir -p /app/hadoop/tmp\n",
    "sudo chown hduser:hadoop /app/hadoop/tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now edit ``core-site.xml`` and ``hdfs-site.xml``. You'll find these files in **$HADOOP_HOME/etc/hadoop** directory.\n",
    "\n",
    "Start with **core-site.xml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo gedit $HADOOP_HOME/etc/hadoop/core-site.xml\n",
    "\n",
    "#paste these lines between <configuration> </configuration> tags\n",
    "<property>\n",
    "  <name>hadoop.tmp.dir</name>\n",
    "  <value>/app/hadoop/tmp</value>\n",
    "  <description>A base for other temporary directories.</description>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "  <name>fs.defaultFS</name>\n",
    "  <value>hdfs://localhost:9000</value>\n",
    "  <description>The name of the default file system.  A URI whose\n",
    "  scheme and authority determine the FileSystem implementation.  The\n",
    "  uri's scheme determines the config property (fs.SCHEME.impl) naming\n",
    "  the FileSystem implementation class.  The uri's authority is used to\n",
    "  determine the host, port, etc. for a filesystem.</description>\n",
    "</property>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hdfs-site.xml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo gedit $HADOOP_HOME/etc/hadoop/hdfs-site.xml\n",
    "\n",
    "#paste these lines between <configuration> </configuration> tags\n",
    "<property>\n",
    "  <name>dfs.replication</name>\n",
    "  <value>1</value>\n",
    "  <description>Default block replication.\n",
    "  The actual number of replications can be specified when the file is created.\n",
    "  The default is used if replication is not specified in create time.\n",
    "  </description>\n",
    "</property>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format namenode (you'll need to do this only the first time you set up hadoop cluster i.e, the time of installation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On running this command, you'll get the o/p with SHUTDOWN_MSG at the end.\n",
    "#Don't worry it's not an error\n",
    "$HADOOP_HOME/bin/hdfs namenode -format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to start the HADOOP CLUSTER!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start the namenode and datanode daemon\n",
    "$HADOOP_HOME/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, it's done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the cluster nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this command\n",
    "jps\n",
    "\n",
    "#also you can browse the namenode web interface on this link\n",
    "http://localhost:9870/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the HDFS directories which we'll need to execute MapReduce jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$HADOOP_HOME/bin/hdfs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stop the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$HADOOP_HOME/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YARN on a single node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the ``.bashrc`` and append other environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo gedit ~/.bashrc\n",
    "\n",
    "#and append the following\n",
    "export HADOOP_MAPRED_HOME=${HADOOP_HOME}\n",
    "export HADOOP_COMMON_HOME=${HADOOP_HOME}\n",
    "export HADOOP_HDFS_HOME=${HADOOP_HOME}\n",
    "export HADOOP_YARN_HOME=${HADOOP_HOME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **mapred-site.xml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo gedit $HADOOP_HOME/etc/hadoop/mapred-site.xml\n",
    "\n",
    "#paste these lines between <configuration> </configuration> tags\n",
    "<property>\n",
    "  <name>mapred.job.tracker</name>\n",
    "  <value>localhost:54311</value>\n",
    "  <description>The host and port that the MapReduce job tracker runs\n",
    "  at.  If \"local\", then jobs are run in-process as a single map\n",
    "  and reduce task.\n",
    "  </description>\n",
    "</property>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**yarn-site.xml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo gedit $HADOOP_HOME/etc/hadoop/yarn-site.xml\n",
    "\n",
    "#paste these lines between <configuration> </configuration> tags\n",
    "<property>\n",
    "        <name>yarn.nodemanager.aux-services</name>\n",
    "        <value>mapreduce_shuffle</value>\n",
    "</property>\n",
    "<property>\n",
    "        <name>yarn.nodemanager.env-whitelist</name>\n",
    "        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>\n",
    "</property>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To check the distributed filesystem working properly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type and check the results for the following commands\n",
    "#list of files and directories on your distributed filesystem\n",
    "hdfs dfs -ls\n",
    "\n",
    "#now create a relative path\n",
    "hdfs dfs -mkdir /user\n",
    "hdfs dfs -mkdir /user/<username>\n",
    "\n",
    "#relative and absolute path\n",
    "hdfs dfs -mkdir /cluster   #----> This directory will be created in your dfs home i.e., where directory user is!\n",
    "\n",
    "hdfs dfs -mkdir cluster    #----> This will be created inside /user/<username>\n",
    "\n",
    "#You can view the added directories in the WebUI too\n",
    "#browse localhost:9870\n",
    "\n",
    "#and check the option Utilities -> Browse filesystem\n",
    "#it displays some webhdfs Server Error [This error is shown for the java versions >=9]\n",
    "#open and edit hadoop-env.sh\n",
    "export HADOOP_OPTS=\"--add-modules java.activation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I hope your hadoop distributed file system is working fine!**\n",
    "\n",
    "**In case of trouble or mistake in the code, notify me!**\n",
    "\n",
    "Now, we can move further and learn the basic commands and their usage in hadoop filesystem. Tutorial is available [HERE](https://github.com/rajatgarg149/BigData-Essentials-HDFS-SPARK-RDD/blob/master/Hadoop%20Tutorial.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
